{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import robomimic.utils.tensor_utils as TensorUtils\n",
    "import robomimic.utils.obs_utils as ObsUtils\n",
    "import robomimic.utils.train_utils as TrainUtils\n",
    "from bc_benchmark_algos.dataset.robomimic import RobomimicDataset\n",
    "from bc_benchmark_algos.rollout_env.robomimic import RobomimicRolloutEnv\n",
    "from robomimic.config import config_factory\n",
    "from robomimic.algo import algo_factory, RolloutPolicy\n",
    "import torch\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test tensor utils\n",
    "x = {\"obs\": {\"agentview_image\": np.random.randn(3, 84, 84)}}\n",
    "x = TensorUtils.to_tensor(x)\n",
    "assert isinstance(x[\"obs\"][\"agentview_image\"], torch.Tensor) \n",
    "x = TensorUtils.to_batch(x)\n",
    "assert x[\"obs\"][\"agentview_image\"].shape == (1, 3, 84, 84)\n",
    "x = TensorUtils.to_sequence(x)\n",
    "assert x[\"obs\"][\"agentview_image\"].shape == (1, 1, 3, 84, 84)\n",
    "x = TensorUtils.repeat_seq(x=x, k=10)\n",
    "assert x[\"obs\"][\"agentview_image\"].shape == (1, 10, 3, 84, 84)\n",
    "x = TensorUtils.slice(x=x, dim=1, start=0, end=5)\n",
    "assert x[\"obs\"][\"agentview_image\"].shape == (1, 5, 3, 84, 84)\n",
    "y = TensorUtils.shift_seq(x=x, k=1)\n",
    "assert torch.equal(x[\"obs\"][\"agentview_image\"][:, 0, :], y[\"obs\"][\"agentview_image\"][:, 1, :])\n",
    "y = TensorUtils.shift_seq(x=x, k=-1)\n",
    "assert torch.equal(x[\"obs\"][\"agentview_image\"], y[\"obs\"][\"agentview_image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs modality: low_dim with keys: ['robot0_eef_pos', 'robot0_eef_quat']\n",
      "using obs modality: rgb with keys: ['agentview_image']\n",
      "using obs modality: depth with keys: []\n",
      "using obs modality: scan with keys: []\n"
     ]
    }
   ],
   "source": [
    "# setup config\n",
    "config_path = \"../config/bc_rnn.json\"\n",
    "dataset_path = \"../../datasets/test/square_ph.hdf5\"\n",
    "output_dir = \"output\"\n",
    "ext_cfg = json.load(open(config_path, 'r'))\n",
    "config = config_factory(ext_cfg[\"algo_name\"])\n",
    "with config.unlocked():\n",
    "    config.update(ext_cfg)\n",
    "config.train.data = dataset_path\n",
    "config.train.output_dir = output_dir\n",
    "config.train.frame_stack = 5\n",
    "config.train.seq_length = 1\n",
    "config.lock()\n",
    "\n",
    "ObsUtils.initialize_obs_utils_with_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobomimicDataset: loading dataset into memory...\n",
      "100%|██████████| 20/20 [00:00<00:00, 202.33it/s]\n",
      "RobomimicDataset: caching get_item calls...\n",
      "100%|██████████| 2989/2989 [00:00<00:00, 3587.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# create validset\n",
    "dataset_path = \"../../datasets/test/square_ph.hdf5\"\n",
    "validset = RobomimicDataset.dataset_factory(\n",
    "    config=config,\n",
    "    obs_group_to_keys=ObsUtils.OBS_GROUP_TO_KEYS[config.algo_name], \n",
    "    filter_by_attribute=\"valid\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Probing, EGL cannot run on this device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created environment with name NutAssemblySquare\n",
      "Action size is 7\n",
      "{'env_name': 'NutAssemblySquare', 'env_version': '1.4.1', 'type': 1, 'env_kwargs': {'has_renderer': False, 'has_offscreen_renderer': True, 'ignore_done': True, 'use_object_obs': True, 'use_camera_obs': True, 'control_freq': 20, 'controller_configs': {'type': 'OSC_POSE', 'input_max': 1, 'input_min': -1, 'output_max': [0.05, 0.05, 0.05, 0.5, 0.5, 0.5], 'output_min': [-0.05, -0.05, -0.05, -0.5, -0.5, -0.5], 'kp': 150, 'damping': 1, 'impedance_mode': 'fixed', 'kp_limits': [0, 300], 'damping_limits': [0, 10], 'position_limits': None, 'orientation_limits': None, 'uncouple_pos_ori': True, 'control_delta': True, 'interpolation': None, 'ramp_ratio': 0.2}, 'robots': ['Panda'], 'camera_depths': False, 'camera_heights': 84, 'camera_widths': 84, 'reward_shaping': False, 'camera_names': ['agentview'], 'render_gpu_device_id': 0}}\n"
     ]
    }
   ],
   "source": [
    "# create rollout env\n",
    "_, _, video_dir = TrainUtils.get_exp_dir(config)\n",
    "rollout_env = RobomimicRolloutEnv(config=config, validset=validset, video_dir=video_dir)\n",
    "print(rollout_env.env_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3, 84, 84)\n",
      "torch.Size([1, 6, 5, 3, 84, 84])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m inputs \u001b[38;5;241m=\u001b[39m rollout_env\u001b[38;5;241m.\u001b[39minputs_from_initial_obs(obs\u001b[38;5;241m=\u001b[39mobs)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magentview_image\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magentview_image\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;241m1\u001b[39m, config\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mframe_stack\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m84\u001b[39m, \u001b[38;5;241m84\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# test inputs_from_initial_obs\n",
    "demo_id = validset.demos[0]\n",
    "initial_state = dict(states=validset.hdf5_file[f\"data/{demo_id}/states\"][0])\n",
    "initial_state[\"model\"] = validset.hdf5_file[f\"data/{demo_id}\"].attrs[\"model_file\"]\n",
    "rollout_env.env.reset()\n",
    "obs = rollout_env.env.reset_to(initial_state)\n",
    "obs, _, _, _ = rollout_env.env.step(np.zeros(7))\n",
    "print(obs[\"agentview_image\"].shape)\n",
    "inputs = rollout_env.inputs_from_initial_obs(obs=obs)\n",
    "print(inputs[\"obs\"][\"agentview_image\"].shape)\n",
    "assert inputs[\"obs\"][\"agentview_image\"].shape == (1, config.train.frame_stack+1, 3, 84, 84)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mental-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
