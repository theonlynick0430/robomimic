{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algo Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground for GPT_Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robomimic.models.transformers import GPT_Backbone\n",
    "import robomimic.utils.tensor_utils as TensorUtils\n",
    "import torch \n",
    "\n",
    "embed_dim = 16\n",
    "context_length = 10\n",
    "\n",
    "gpt = GPT_Backbone(embed_dim=embed_dim, context_length=context_length)\n",
    "print(gpt)\n",
    "\n",
    "x = TensorUtils.to_batch(torch.randn((context_length, embed_dim)))\n",
    "y = gpt(x)\n",
    "print(\"output shape:\")\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground for RNN_Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robomimic.models.base_nets import RNN_Base, MLP\n",
    "import torch\n",
    "\n",
    "batch_size = 16\n",
    "seq_len = 10\n",
    "input_dim = 20 \n",
    "output_dim = 20\n",
    "hidden_dim = 100\n",
    "\n",
    "per_step_net = MLP(input_dim=hidden_dim, output_dim=output_dim, layer_dims=(25,))\n",
    "# create stacked LSTM\n",
    "rnn = RNN_Base(input_dim=input_dim, rnn_hidden_dim=hidden_dim, rnn_num_layers=2, per_step_net=per_step_net)\n",
    "print(rnn)\n",
    "\n",
    "x = torch.randn((batch_size, seq_len, input_dim))\n",
    "h0, c0 = torch.randn((2, batch_size, hidden_dim)), torch.randn((2, batch_size, hidden_dim))\n",
    "y, state = rnn(x, (h0, c0), return_state=True)\n",
    "h, c = state\n",
    "print(\"output shape:\")\n",
    "print(y.shape)\n",
    "print(\"hidden state shape:\")\n",
    "print(h.shape)\n",
    "print(\"context state shape:\")\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground for ObservationEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robomimic.models.obs_nets import ObservationEncoder\n",
    "import robomimic.utils.tensor_utils as TensorUtils\n",
    "import torch\n",
    "\n",
    "obs_encoder = ObservationEncoder()\n",
    "\n",
    "\n",
    "# register rgb encoder\n",
    "rgb_input_dim = (3, 224, 224)\n",
    "net_kwargs = {\n",
    "    \"input_shape\": rgb_input_dim, # don't include batch_size in input_shape\n",
    "    \"backbone_class\": \"ResNet18Conv\", # backbone \n",
    "    \"backbone_kwargs\": {\"pretrained\": True, \"input_coord_conv\": False},\n",
    "    \"pool_class\": \"SpatialSoftmax\", # maps features to embedding of shape (batch_size, num_kp, 2)\n",
    "    \"pool_kwargs\": {\"num_kp\": 32},\n",
    "    \"flatten\": True, # default value, flattens embedding to shape (batch_size, num_kp*2)\n",
    "    \"feature_dimension\": 64 # default value, projects embedding to shape (batch_size, feature_dimension)\n",
    "}\n",
    "obs_encoder.register_obs_key(\n",
    "    name=\"rgb\",\n",
    "    shape=rgb_input_dim,\n",
    "    net_class=\"VisualCore\", # combines visual backbone with pooling\n",
    "    net_kwargs=net_kwargs,\n",
    ")\n",
    "\n",
    "# register low_dim encoder (ee_pos, ee_quat, etc)\n",
    "proprio_input_dim = 7\n",
    "obs_encoder.register_obs_key(\n",
    "    name=\"low_dim\",\n",
    "    shape=proprio_input_dim,\n",
    ") # if we don't specify net_class, obs is flattened and concatenated\n",
    "\n",
    "obs_encoder.make()\n",
    "\n",
    "x = dict()\n",
    "x[\"rgb\"] = TensorUtils.to_batch(torch.rand(rgb_input_dim))\n",
    "x[\"low_dim\"] = TensorUtils.to_batch(torch.rand(proprio_input_dim))\n",
    "y = obs_encoder(x)\n",
    "# concat (1, 64) and (1, 7) -> (1, 71)\n",
    "print(\"encoded state shape:\")\n",
    "print(y.shape)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mental-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
