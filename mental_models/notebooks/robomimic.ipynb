{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (320, 512)\n",
    "GOAL_PROMPT = {\n",
    "    \"can_ph\": \"Pick up the red can and place it in the bottom right container.\", \n",
    "    \"lift_ph\": \"Pick up the red block.\",\n",
    "    \"square_ph\": \"Pick up the square tool and place it on the square peg.\", \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"robomimic_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobomimicDataset(Dataset):\n",
    "    def __init__(self, data_dir, cam_views=[\"agentview\"], max_demos=50, max_samples=50):\n",
    "        \"\"\"\n",
    "        Initializes a general Pytorch dataset for Robomimic data.\n",
    "\n",
    "        Args:\n",
    "            data_dir (str): Directory that contains Robomimic datasets (.hdf5 files).\n",
    "            cam_views ([str]): Only images from these camera views will be extracted.\n",
    "            max_demos (int): Maximum number of demos per dataset to be extracted.\n",
    "            max_samples (int): Maximum number of samples per demo to be extracted.\n",
    "        \"\"\"\n",
    "        # N = demos*samples\n",
    "        self.data_dir = data_dir\n",
    "        # (N, 7) - ee_pos, ee_quat\n",
    "        self.state = []\n",
    "        # (len(cam_views), N, H, W, 3)\n",
    "        self.rgb = [[] for _ in range(len(cam_views))]\n",
    "        # (len(cam_views), N, H, W, 1)\n",
    "        self.depth = [[] for _ in range(len(cam_views))]\n",
    "        # (N, 7) - delta ee_pos, delta ee_aa, grip_cmd\n",
    "        self.action = []\n",
    "        # (N,) - idx into GOAL_PROMPT\n",
    "        self.goal_key = []\n",
    "\n",
    "        for dataset_filename in os.listdir(self.data_dir):\n",
    "            dataset_path = os.path.join(self.data_dir, dataset_filename)\n",
    "            dataset = os.path.splitext(os.path.basename(dataset_filename))[0]\n",
    "            logger.info(f\"loading data from {dataset}...\")\n",
    "            f = h5py.File(dataset_path, \"r\")\n",
    "            demos = list(f[\"data\"].keys())\n",
    "            for i, demo in enumerate(demos):\n",
    "                if i == max_demos:\n",
    "                    break\n",
    "                # demo\n",
    "                demo_grp = f[\"data/{}\".format(demo)]\n",
    "                num_samples = demo_grp.attrs[\"num_samples\"]\n",
    "                num_samples_trunc = min(num_samples, max_samples)\n",
    "                sample_idx = np.linspace(0, num_samples-1, num_samples_trunc, dtype=int)\n",
    "                # goal prompt\n",
    "                self.goal_key.extend([dataset for _ in range (num_samples_trunc)])\n",
    "                # action\n",
    "                action = list(demo_grp[\"actions\"][sample_idx])\n",
    "                self.action.extend(action)\n",
    "                # state\n",
    "                eef_pos = demo_grp[\"obs/{}\".format(\"robot0_eef_pos\")][sample_idx]\n",
    "                eef_quat = demo_grp[\"obs/{}\".format(\"robot0_eef_quat\")][sample_idx]\n",
    "                state = list(np.concatenate((eef_pos, eef_quat), axis=-1))\n",
    "                self.state.extend(state)\n",
    "                # rgb, depth\n",
    "                for j, cam_view in enumerate(cam_views):\n",
    "                    rgb_seq = list(demo_grp[\"obs/{}\".format(cam_view+\"_image\")][sample_idx])\n",
    "                    depth_seq = list(demo_grp[\"obs/{}\".format(cam_view+\"_depth\")][sample_idx])\n",
    "                    self.rgb[j].extend(rgb_seq)\n",
    "                    self.depth[j].extend(depth_seq)\n",
    "                        \n",
    "        self.state = np.array(self.state)\n",
    "        self.rgb = np.array(self.rgb)\n",
    "        self.depth = np.array(self.depth)\n",
    "        self.action = np.array(self.action)\n",
    "        self.goal_key = np.array(self.goal_key)\n",
    "            \n",
    "    def __getitem__(self, index): \n",
    "        # [insert pre-processing steps here for particular use case]\n",
    "        return (self.state[index],\n",
    "                self.rgb[:, index, :],\n",
    "                self.depth[:, index, :],\n",
    "                self.action[index], \n",
    "                GOAL_PROMPT[self.goal_key[index]])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.state)\n",
    "    \n",
    "dataset = RobomimicDataset(data_dir=\"../data/svd_sample\")\n",
    "logger.info(dataset.state.shape)\n",
    "logger.info(dataset.action.shape)\n",
    "logger.info(dataset.goal_key.shape)\n",
    "logger.info(dataset.rgb.shape)\n",
    "logger.info(dataset.depth.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mental-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
